{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 9\n",
    "\n",
    "Networks and Systems Security\n",
    "Week 07\n",
    "Gen AI Security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Aims of the Seminar\n",
    "\n",
    "This laboratory exercise introduces you to fundamental security\n",
    "considerations in generative artificial intelligence. Students will deploy a\n",
    "local large language model using Ollama and investigate key\n",
    "vulnerabilities, threat categories, and defensive strategies highlighted in\n",
    "the lecture material. The aim is to develop awareness of risks inherent in\n",
    "generative systems and to practise core security evaluation procedures\n",
    "in a controlled environment.\n",
    "\n",
    "Learning Objectives:\n",
    "- Demonstrate the ability to run a local LLM instance and evaluate\n",
    "its behaviour under controlled security-testing scenarios.\n",
    "- Identify major generative AI threat vectors, including prompt\n",
    "injection, data poisoning, inversion, model theft, and adversarial\n",
    "manipulation.\n",
    "- Apply basic red-teaming principles to assess the robustness and\n",
    "safety of a deployed model.\n",
    "- Recommend high-level mitigation strategies based on security\n",
    "best practices for model deployment, governance, and incident\n",
    "response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\kierm\\miniconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\kierm\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n",
    "\n",
    "# !ollama -version\n",
    "# Since it didnt work i installed it directly from the docs instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search all available models on Ollama website via: https://ollama.com/search \n",
    "(Chose llava: https://ollama.com/library/llava)\n",
    "\n",
    "Review Local LLMs leadership dashboard on hugging Face ðŸ¤—:  https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=0%2C8&official=true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I â€” Deploying a Local Model with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.13.1\n"
     ]
    }
   ],
   "source": [
    "# !ollama pull smollm2:135m\n",
    "# Since it didnt work i installed it directly from the docs instead\n",
    "\n",
    "!ollama -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llava', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Why is the sky blue?',\n",
    "    },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
